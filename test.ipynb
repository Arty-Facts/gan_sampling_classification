{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "from medmnist import INFO\n",
    "import medmnist\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST torch.Size([32]) tensor(0.) tensor(1.) tensor(0.1273) tensor(0.3038)\n",
      "FashionMNIST torch.Size([32]) tensor(0.) tensor(1.) tensor(0.2998) tensor(0.3535)\n",
      "SVHN torch.Size([32]) tensor(0.) tensor(1.) tensor(0.4294) tensor(0.1935)\n",
      "CIFAR10 torch.Size([32]) tensor(0.) tensor(1.) tensor(0.4997) tensor(0.2469)\n",
      "CIFAR100 torch.Size([32]) tensor(0.) tensor(1.) tensor(0.4740) tensor(0.2678)\n",
      "PathMNIST torch.Size([32]) tensor(0.0314) tensor(1.) tensor(0.6696) tensor(0.1804)\n",
      "DermaMNIST torch.Size([32]) tensor(0.0118) tensor(1.) tensor(0.6450) tensor(0.1654)\n",
      "BloodMNIST torch.Size([32]) tensor(0.0196) tensor(1.) tensor(0.7174) tensor(0.2116)\n",
      "BreastMNIST torch.Size([32]) tensor(0.0039) tensor(0.9686) tensor(0.3292) tensor(0.2070)\n",
      "RetinaMNIST torch.Size([32]) tensor(0.) tensor(1.) tensor(0.2769) tensor(0.2595)\n",
      "OrganCMNIST torch.Size([32]) tensor(0.) tensor(1.) tensor(0.5011) tensor(0.3117)\n"
     ]
    }
   ],
   "source": [
    "for dataset in ds.ALL_DATASETS:\n",
    "    data = ds.get_dataset(dataset, \"data\",reduce_level=1)\n",
    "    # imgs = torch.stack([img for img, _ in data[\"train\"]])\n",
    "    # labels = data[\"train\"].lables\n",
    "    # print(dataset, data[\"train\"].get_random_index_for_label(1))#, imgs.min(), imgs.max(), imgs.mean(), imgs.std(), imgs.shape, labels.shape)\n",
    "    dataloader = DataLoader(dataset=data[\"train\"], batch_sampler=ds.InfiniteClassSampler(data[\"train\"], batch_size=32, balanced=True))\n",
    "    samples = 0\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        # Training step...\n",
    "        print(dataset, labels.shape, images.min(), images.max(), images.mean(), images.std())\n",
    "        # labels = labels.argmax(dim=1)\n",
    "        \n",
    "        # balence = [\n",
    "        #     len(torch.where(labels == i)[0]) for i in range(data[\"train\"].label_shape)\n",
    "        # ]\n",
    "        # samples += images.shape[0]\n",
    "        # print(samples, dataset, balence, labels.shape)\n",
    "        # if step == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomAutocontrast(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5]),\n",
    "    ])\n",
    "img = torch.rand((4, 1, 224, 224))\n",
    "\n",
    "print(data_transform(img).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import INFO\n",
    "import medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_class': 'OrganCMNIST',\n",
       " 'description': 'The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.',\n",
       " 'url': 'https://zenodo.org/records/10519652/files/organcmnist.npz?download=1',\n",
       " 'MD5': 'b9ceb9546e10131b32923c5bbeaea2b1',\n",
       " 'url_64': 'https://zenodo.org/records/10519652/files/organcmnist_64.npz?download=1',\n",
       " 'MD5_64': '3ce34a8724ea6f548e6db4744d03b6a9',\n",
       " 'url_128': 'https://zenodo.org/records/10519652/files/organcmnist_128.npz?download=1',\n",
       " 'MD5_128': '773c1f009daa3fe5d9a2a201b2a7ed94',\n",
       " 'url_224': 'https://zenodo.org/records/10519652/files/organcmnist_224.npz?download=1',\n",
       " 'MD5_224': '050f5e875dc056f6768abf94ec9995d1',\n",
       " 'task': 'multi-class',\n",
       " 'label': {'0': 'bladder',\n",
       "  '1': 'femur-left',\n",
       "  '2': 'femur-right',\n",
       "  '3': 'heart',\n",
       "  '4': 'kidney-left',\n",
       "  '5': 'kidney-right',\n",
       "  '6': 'liver',\n",
       "  '7': 'lung-left',\n",
       "  '8': 'lung-right',\n",
       "  '9': 'pancreas',\n",
       "  '10': 'spleen'},\n",
       " 'n_channels': 1,\n",
       " 'n_samples': {'train': 12975, 'val': 2392, 'test': 8216},\n",
       " 'license': 'CC BY 4.0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INFO['organcmnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\n",
    "    1, 2, 3 if False else 0, 4\n",
    "]\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
